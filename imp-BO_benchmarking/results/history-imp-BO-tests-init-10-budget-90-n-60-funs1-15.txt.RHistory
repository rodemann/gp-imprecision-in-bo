design = data$data
response = data$target
precise_GP_fit <- DiceKriging::km(response = response, design = design, covtype = base_kernel,
estim.method = "MLE", optim.method = "gen",
nugget = 1e-8*var(response), control = list(trace = FALSE))
## add precise GP model to plot
test_points = grid
precise_GP <- predict(precise_GP_fit, newdata = test_points, type = "response", se.compute = FALSE)
pred_grid = cbind(test_points, precise_GP$mean)
lines(pred_grid)
## playground:
fun = smoof::makeAlpine02Function(1)
x = runif(100,-10,10) %>% sort() %>% as.vector()
y = sapply(x, FUN = fun) %>% as.vector()
data = data.frame(x=x, y=y)
base_kernel = "powexp"
.task = makeRegrTask(data = data, target = "y")
lrn = makeLearner("igp_upper_mean", base_kernel = base_kernel, imprecision = 100, predict.type = "se")
mod_upper = train(lrn, .task)
lrn = makeLearner("igp_lower_mean", base_kernel = base_kernel, imprecision = 100, predict.type = "se")
mod_lower = train(lrn, .task)
y = sapply(x, FUN = fun) %>% as.vector()
data = data.frame(x=x, y=y)
.task = makeRegrTask(data = data, target = "y")
y
x = runif(100,-4,4) %>% sort() %>% as.vector()
y = sapply(x, FUN = fun) %>% as.vector()
data = data.frame(x=x, y=y)
.task = makeRegrTask(data = data, target = "y")
fun
## playground:
fun = smoof::makeAlpine02Function(1)
x = runif(100,0,10) %>% sort() %>% as.vector()
y = sapply(x, FUN = fun) %>% as.vector()
data = data.frame(x=x, y=y)
.task = makeRegrTask(data = data, target = "y")
base_kernel = "powexp"
lrn = makeLearner("igp_upper_mean", base_kernel = base_kernel, imprecision = 100, predict.type = "se")
mod_upper = train(lrn, .task)
lrn = makeLearner("igp_lower_mean", base_kernel = base_kernel, imprecision = 100, predict.type = "se")
mod_lower = train(lrn, .task)
#test_data = subset(data, select = x)
test_data = data.frame(x = seq(-4,4,0.005), row.names = NULL)
#viz results
grid = seq(-4,4,0.005)
#lower
preds = predict(mod_lower, newdata = test_data)
preds_raw = preds$data$response
plot(x = grid , y = preds_raw, type = "l", col = "steelblue")
preds_raw = preds$data$response
lines(x = grid , y = preds_raw, type = "l", col = "red")
#upper
preds = predict(mod_upper, newdata = test_data)
warnings()
#add training data points
truth = sapply(x, fun) %>% as.vector()
points(x = x, y = truth)
# compare with precise GP model
data = getTaskData(.task, target.extra = TRUE)
design = data$data
response = data$target
#viz results
grid = seq(0,10,0.005)
#lower
preds = predict(mod_lower, newdata = test_data)
preds_raw = preds$data$response
plot(x = grid , y = preds_raw, type = "l", col = "steelblue")
#upper
preds = predict(mod_upper, newdata = test_data)
preds_raw = preds$data$response
lines(x = grid , y = preds_raw, type = "l", col = "red")
#viz results
grid = seq(0,10,0.1)
#lower
preds = predict(mod_lower, newdata = test_data)
grid = seq(0,10,0.1)
test_data = data.frame(x = grid, row.names = NULL)
##viz results
#lower
preds = predict(mod_lower, newdata = test_data)
preds_raw = preds$data$response
plot(x = grid , y = preds_raw, type = "l", col = "steelblue")
#upper
preds = predict(mod_upper, newdata = test_data)
preds_raw = preds$data$response
lines(x = grid , y = preds_raw, type = "l", col = "red")
#add training data points
truth = sapply(x, fun) %>% as.vector()
points(x = x, y = truth)
# compare with precise GP model
data = getTaskData(.task, target.extra = TRUE)
design = data$data
response = data$target
precise_GP_fit <- DiceKriging::km(response = response, design = design, covtype = base_kernel,
estim.method = "MLE", optim.method = "gen",
nugget = 1e-8*var(response), control = list(trace = FALSE))
lrn = makeLearner("igp_upper_mean", base_kernel = base_kernel, imprecision = 1000, predict.type = "se")
## playground:
fun = smoof::makeAlpine02Function(1)
x = runif(100,0,10) %>% sort() %>% as.vector()
y = sapply(x, FUN = fun) %>% as.vector()
data = data.frame(x=x, y=y)
.task = makeRegrTask(data = data, target = "y")
base_kernel = "powexp"
lrn = makeLearner("igp_upper_mean", base_kernel = base_kernel, imprecision = 1000, predict.type = "se")
mod_upper = train(lrn, .task)
mod_lower = train(lrn, .task)
lrn = makeLearner("igp_lower_mean", base_kernel = base_kernel, imprecision = 100, predict.type = "se")
test_data = data.frame(x = grid, row.names = NULL)
grid = seq(0,10,0.1)
##viz results
#lower
preds = predict(mod_lower, newdata = test_data)
preds_raw = preds$data$response
plot(x = grid , y = preds_raw, type = "l", col = "steelblue")
#upper
preds = predict(mod_upper, newdata = test_data)
preds_raw = preds$data$response
lines(x = grid , y = preds_raw, type = "l", col = "red")
points(x = x, y = truth)
#add training data points
truth = sapply(x, fun) %>% as.vector()
# compare with precise GP model
data = getTaskData(.task, target.extra = TRUE)
design = data$data
response = data$target
precise_GP_fit <- DiceKriging::km(response = response, design = design, covtype = base_kernel,
estim.method = "MLE", optim.method = "gen",
nugget = 1e-8*var(response), control = list(trace = FALSE))
precise_GP <- predict(precise_GP_fit, newdata = test_points, type = "response", se.compute = FALSE)
pred_grid = cbind(test_points, precise_GP$mean)
## add precise GP model to plot
test_points = grid
lines(pred_grid)
grid
pred_grid
grid
test_points
precise_GP <- predict(precise_GP_fit, newdata = test_points, type = "response", se.compute = FALSE)
pred_grid = cbind(test_points, precise_GP$mean)
pred_grid
lines(pred_grid)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
## playground:
fun = smoof::makeChungReynoldsFunction(1)
plot(fun)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
warnings()
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
warnings()
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
preds_raw
preds$data$se
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
preds$data$se
debugonce(predictLearner.igp_upper_mean)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
predictLearner.igp_lower_mean()
debugonce(predictLearner.igp_upper_mean)
#upper
preds = predict(mod_upper, newdata = test_data)
debugonce(igp_upper_mean_se_scalar)
debugonce(igp_upper_mean_se_scalar)
debugonce(predictLearner.igp_upper_mean)
#upper
preds = predict(mod_upper, newdata = test_data)
debugonce(igp_upper_mean_se_scalar)
debugonce(predictLearner.igp_upper_mean)
#upper
preds = predict(mod_upper, newdata = test_data)
debugonce(igp_upper_mean_se_scalar)
scalar
kernel_vector
kernel_null
theta
power
design
upper_mean_var
upper_mean_se
Q
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_upper_mean.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_upper_mean.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_playground.R', echo=TRUE)
warnings()
upper_mean_se
upper_mean_se
upper_mean_var
sqrt(upper_mean_var)
upper_mean_se
kernel_null
scalar
kernel_vector
Kernel_matrix_n_inverse
t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector)
kernel_null
t(kernel_vector) %*% sum_rows_k)^2
(1 - t(kernel_vector) %*% sum_rows_k)^2
upper_mean_se
upper_mean_var
(kernel_null - (t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector))
+ ((1 - t(kernel_vector) %*% sum_rows_k)^2 / sum_all_k))
((1 - t(kernel_vector) %*% sum_rows_k)^2 / sum_all_k))
((1 - t(kernel_vector) %*% sum_rows_k)^2 / sum_all_k)
(t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector))
(kernel_null - (t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector))
)
(kernel_null - (t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector)))
kernel_null
(t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector))
sprintf("%.100f",kernel_null)
sprintf("%.100f", (t(kernel_vector) %*% Kernel_matrix_n_inverse %*% as.matrix(kernel_vector)))
upper_mean_var
sqrt(0)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_lower_mean.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_lower_mean.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_upper_mean.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imprecise-bayes-opt-parallel/igp_upper_mean.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imbo-bo-benchmarking-test-errors-in-igp-se-pred.R', echo=TRUE)
save(results_list, file = paste(getwd(),"/imp-BO_benchmarking/results-imp-BO-tests-init-10-budget-90-n-40-funs1-5" ,sep=""))
save(results_list, file = paste(getwd(),"/imp-BO_benchmarking/results/results-imp-BO-tests-init-10-budget-90-n-40-funs1-5" ,sep=""))
save(results_list, file = paste(getwd(),"/imp-BO_benchmarking/results/results-imp-BO-tests-init-10-budget-90-n-40-funs1-5" ,sep=""))
# divide into chunks
synth_test_functions <- all_benchmark_funs[1:15]
if(total_iters <= budget){
#print("budget met")
}else
warning("budget not met")
# batch bo with multiple imprecision degrees and 3 batches
imprecision_degree <- c(1,10,100)
number_of_batches <- 3
number_of_models_batch_1 <- 2 * length(imprecision_degree) + 1
number_of_models_batch_2 <- (number_of_models_batch_1 - 1)/2
number_of_models_batch_3 <- (number_of_models_batch_2 - 1)/2
number_of_models_per_batch <- c(number_of_models_batch_1,
number_of_models_batch_2,
number_of_models_batch_3)
# set iterations
# max_iters_per_batch <- floor(budget/(number_of_models_per_batch))
# decide for # of iters in each batch
iters_per_batch_per_model <- c(2,2,10)
# check whether setting is allowed
total_iters <- sum(number_of_models_per_batch * iters_per_batch_per_model)
# set iterations
max_iters_per_batch <- floor(budget/(number_of_models_per_batch))
max_iters_per_batch
number_of_models_per_batch
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
warnings()
fun
parallel_imp_bo_res <- imp_BO_univariate(fun = fun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree)
ctrl
fun <- synth_test_functions[[i]]
fun
parameter_set <- getParamSet(fun)
# set Control Argument of BO
ctrl <- makeMBOControl(propose.points = 1L)
# iters = budget for classic bo
ctrl <- setMBOControlTermination(ctrl, iters = budget)
#use EI globally as infill crit (expect for GLCB of course)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(), opt = "focussearch",
opt.focussearch.points = 200, opt.focussearch.maxit = 2)
# same design for all approaches
design <- generateDesign(n = init_design, par.set = parameter_set, fun = lhs::randomLHS)
# classic bo with classic GP with powexp
lrn_classic <- makeLearner("regr.km", covtype = "powexp", predict.type = "se", optim.method = "gen",
control = list(trace = FALSE), config = list(on.par.without.desc = "warn"))
# ensure numerical stability in km {DiceKriging} cf. github issue and recommendation by Bernd Bischl
y = apply(design, 1, fun)
Nuggets = 1e-8*var(y)
lrn_classic = setHyperPars(learner = lrn_classic, nugget=Nuggets)
#start classic bo
cl_bo_res <- mbo(fun, design, ctrl, learner = lrn_classic, show.info = FALSE)
budget = 9
# parallel bo with one imprecision degree
imprecision_degree <- 100
ctrl <- setMBOControlTermination(ctrl, iters = floor(budget/3))
parallel_imp_bo_res <- imp_BO_univariate(fun = fun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree)
parallel_imp_bo_res
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
Sys.time()
print(Sys.time())
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
# save console output of experiments here
sink("results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
# save console output of experiments here
sink("imp-BO_benchmarking/results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
# save console output of experiments here
sink("imp-BO_benchmarking/results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
# save console output of experiments here
sink("imp-BO_benchmarking/results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
source('~/master-thesis-r/imp-BO_benchmarking/imp-BO-synthetic-benchmarking-1.R', echo=TRUE)
warnings
warnings()
# save console output of experiments here
sink("imp-BO_benchmarking/results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
# divide into chunks
synth_test_functions <- all_benchmark_funs[1:15]
number_of_funs <- length(synth_test_functions)
# define sample size (number of BO runs per test function)
n <- 60
# define total Budget of evaluations per BO run
budget <- 90
# define initial design size
init_design <- 10
# list that is used inside foreach
results_list <- list()
for (i in 1:number_of_funs) {
i
sink()
fun <- synth_test_functions[[i]]
parameter_set <- getParamSet(fun)
results_one_fun <- list()
for (j in 1:n) {
j
sink()
# same design for all approaches
design <- generateDesign(n = init_design, par.set = parameter_set, fun = lhs::randomLHS)
# set Control Argument of BO
ctrl <- makeMBOControl(propose.points = 1L)
# iters = budget for classic bo
ctrl <- setMBOControlTermination(ctrl, iters = budget)
#use EI globally as infill crit (expect for GLCB of course)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(), opt = "focussearch",
opt.focussearch.points = 200, opt.focussearch.maxit = 2)
## classic bo with classic GP with powexp
lrn_classic <- makeLearner("regr.km", covtype = "powexp", predict.type = "se", optim.method = "gen",
control = list(trace = FALSE), config = list(on.par.without.desc = "warn"))
# ensure numerical stability in km {DiceKriging} cf. github issue and recommendation by Bernd Bischl
y = apply(design, 1, fun)
Nuggets = 1e-8*var(y)
lrn_classic = setHyperPars(learner = lrn_classic, nugget=Nuggets)
#start classic bo
#cl_bo_res <- mbo(fun, design, ctrl, learner = lrn_classic, show.info = FALSE)
# parallel bo with one imprecision degree
imprecision_degree <- 100
ctrl <- setMBOControlTermination(ctrl, iters = floor(budget/3))
tryCatch({
parallel_imp_bo_res <- imp_BO_univariate(fun = tfun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree)
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}
)
sink()
# batch bo with multiple imprecision degrees and 3 batches
imprecision_degree <- c(1,10,100)
number_of_batches <- 3
number_of_models_batch_1 <- 2 * length(imprecision_degree) + 1
number_of_models_batch_2 <- (number_of_models_batch_1 - 1)/2
number_of_models_batch_3 <- (number_of_models_batch_2 - 1)/2
number_of_models_per_batch <- c(number_of_models_batch_1,
number_of_models_batch_2,
number_of_models_batch_3)
# set iterations
#max_iters_per_batch <- floor(budget/(number_of_models_per_batch))
# decide for # of iters in each batch
iters_per_batch_per_model <- c(6,10,18)
# check whether setting is allowed
total_iters <- sum(number_of_models_per_batch * iters_per_batch_per_model)
if(total_iters <= budget){
#print("budget met")
}else
warning("budget not met")
tryCatch({
batch_imp_bo_res <- imp_BO_univariate_batch(fun = fun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree,
number_of_batches = number_of_batches,
iters_per_batch_per_model = iters_per_batch_per_model)
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}
)
# plug-in-bo with Generalized Lower Confidence Bound
# set Control Argument of BO
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritGLCB()) # defaults to lambda=1, rho = 1, imprecision = 100
ctrl <- setMBOControlTermination(ctrl, iters = budget) # no parallelization
glcb_bo_res <- mbo(fun, design, ctrl, learner = NULL, show.info = FALSE)
opt_paths_raw <- list(getOptPathY(cl_bo_res[["opt.path"]]), parallel_imp_bo_res$opt_path_y,
batch_imp_bo_res$opt_path_y_global, getOptPathY(glcb_bo_res[["opt.path"]]))
# ATTENTION only for minimization (no problem here, since we restrict our tests to it)
opt_paths <- lapply(opt_paths_raw, function(proposals){
for (o in 2:length(proposals)) {
if(proposals[o] > proposals[o - 1])
proposals[o] = proposals[o - 1]
}
proposals
})
results_one_fun[[j]] <- opt_paths
}
results_list[[i]] <- results_one_fun
}
# save console output of experiments here
sink("imp-BO_benchmarking/results/console-output-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt")
# divide into chunks
synth_test_functions <- all_benchmark_funs[1:15]
number_of_funs <- length(synth_test_functions)
# define sample size (number of BO runs per test function)
n <- 60
# define total Budget of evaluations per BO run
budget <- 90
# define initial design size
init_design <- 10
# list that is used inside foreach
results_list <- list()
for (i in 1:number_of_funs) {
i
sink()
fun <- synth_test_functions[[i]]
parameter_set <- getParamSet(fun)
results_one_fun <- list()
for (j in 1:n) {
j
sink()
# same design for all approaches
design <- generateDesign(n = init_design, par.set = parameter_set, fun = lhs::randomLHS)
# set Control Argument of BO
ctrl <- makeMBOControl(propose.points = 1L)
# iters = budget for classic bo
ctrl <- setMBOControlTermination(ctrl, iters = budget)
#use EI globally as infill crit (expect for GLCB of course)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(), opt = "focussearch",
opt.focussearch.points = 200, opt.focussearch.maxit = 2)
## classic bo with classic GP with powexp
lrn_classic <- makeLearner("regr.km", covtype = "powexp", predict.type = "se", optim.method = "gen",
control = list(trace = FALSE), config = list(on.par.without.desc = "warn"))
# ensure numerical stability in km {DiceKriging} cf. github issue and recommendation by Bernd Bischl
y = apply(design, 1, fun)
Nuggets = 1e-8*var(y)
lrn_classic = setHyperPars(learner = lrn_classic, nugget=Nuggets)
#start classic bo
#cl_bo_res <- mbo(fun, design, ctrl, learner = lrn_classic, show.info = FALSE)
# parallel bo with one imprecision degree
imprecision_degree <- 100
ctrl <- setMBOControlTermination(ctrl, iters = floor(budget/3))
tryCatch({
parallel_imp_bo_res <- imp_BO_univariate(fun = tfun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree)
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n"); sink()}
)
# batch bo with multiple imprecision degrees and 3 batches
imprecision_degree <- c(1,10,100)
number_of_batches <- 3
number_of_models_batch_1 <- 2 * length(imprecision_degree) + 1
number_of_models_batch_2 <- (number_of_models_batch_1 - 1)/2
number_of_models_batch_3 <- (number_of_models_batch_2 - 1)/2
number_of_models_per_batch <- c(number_of_models_batch_1,
number_of_models_batch_2,
number_of_models_batch_3)
# set iterations
#max_iters_per_batch <- floor(budget/(number_of_models_per_batch))
# decide for # of iters in each batch
iters_per_batch_per_model <- c(6,10,18)
# check whether setting is allowed
total_iters <- sum(number_of_models_per_batch * iters_per_batch_per_model)
if(total_iters <= budget){
#print("budget met")
}else
warning("budget not met")
tryCatch({
batch_imp_bo_res <- imp_BO_univariate_batch(fun = fun, design = design, control = ctrl,
base_kernel = "powexp", imprecision_degree = imprecision_degree,
number_of_batches = number_of_batches,
iters_per_batch_per_model = iters_per_batch_per_model)
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")}
)
# plug-in-bo with Generalized Lower Confidence Bound
# set Control Argument of BO
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritGLCB()) # defaults to lambda=1, rho = 1, imprecision = 100
ctrl <- setMBOControlTermination(ctrl, iters = budget) # no parallelization
glcb_bo_res <- mbo(fun, design, ctrl, learner = NULL, show.info = FALSE)
opt_paths_raw <- list(getOptPathY(cl_bo_res[["opt.path"]]), parallel_imp_bo_res$opt_path_y,
batch_imp_bo_res$opt_path_y_global, getOptPathY(glcb_bo_res[["opt.path"]]))
# ATTENTION only for minimization (no problem here, since we restrict our tests to it)
opt_paths <- lapply(opt_paths_raw, function(proposals){
for (o in 2:length(proposals)) {
if(proposals[o] > proposals[o - 1])
proposals[o] = proposals[o - 1]
}
proposals
})
results_one_fun[[j]] <- opt_paths
}
results_list[[i]] <- results_one_fun
}
?saveHistory
savehistory()
savehistory
# save console output
savehistory(file = "imp-BO_benchmarking/results/history-imp-BO-tests-init-10-budget-90-n-60-funs1-15.txt.RHistory")
